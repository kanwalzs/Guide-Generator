{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Dynamic Tables\n",
    "\n",
    "## Overview\n",
    "\n",
    "This template demonstrates the power of Snowflake Dynamic Tables through a practical e-commerce analytics scenario. You'll learn how to create, monitor, and manage self-refreshing materialized views that automatically update as source data changes, eliminating the need for complex scheduling and manual maintenance.\n",
    "\n",
    "**What you'll build:**\n",
    "- Real-time e-commerce analytics with automatic data freshness\n",
    "- Layered Dynamic Tables with intelligent dependency management  \n",
    "- Monitoring and management capabilities for operational excellence\n",
    "- Cost-effective refresh strategies and best practices\n",
    "\n",
    "**Key Benefits:**\n",
    "- âœ… Automatic data freshness without complex scheduling\n",
    "- âœ… Intelligent dependency management between tables\n",
    "- âœ… Built-in monitoring and management capabilities\n",
    "- âœ… Cost-effective refresh strategies\n",
    "- âœ… Seamless data sharing integration\n",
    "\n",
    "**Business Scenario:** An e-commerce company needs real-time analytics on customer orders and product performance. Traditional batch processing creates data staleness issues, while manual refresh scheduling is error-prone and resource-intensive. Dynamic Tables solve this by automatically maintaining fresh, query-ready data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Let's start by setting up our environment with the standard Snowflake learning configuration and creating a unique schema for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "SETUP_ENVIRONMENT"
   },
   "outputs": [],
   "source": [
    "-- Setup standard learning environment\n",
    "USE ROLE SNOWFLAKE_LEARNING_ROLE;\n",
    "USE WAREHOUSE SNOWFLAKE_LEARNING_WH;\n",
    "USE DATABASE SNOWFLAKE_LEARNING_DB;\n",
    "\n",
    "-- Create unique schema for this demo\n",
    "SET schema_name = CONCAT(CURRENT_USER(), '_DYNAMIC_TABLE');\n",
    "CREATE SCHEMA IF NOT EXISTS IDENTIFIER($schema_name);\n",
    "USE SCHEMA IDENTIFIER($schema_name);\n",
    "\n",
    "-- Clean up any existing objects from previous runs\n",
    "DROP DYNAMIC TABLE IF EXISTS DT_HOURLY_ORDERS;\n",
    "DROP DYNAMIC TABLE IF EXISTS DT_DAILY_SUMMARY;\n",
    "DROP DYNAMIC TABLE IF EXISTS DT_CUSTOMER_METRICS;\n",
    "DROP VIEW IF EXISTS PUBLIC_DAILY_METRICS;\n",
    "DROP TABLE IF EXISTS ORDERS;\n",
    "DROP TABLE IF EXISTS PRODUCTS;\n",
    "DROP TABLE IF EXISTS CUSTOMERS;\n",
    "\n",
    "SELECT 'Environment setup complete. Using schema: ' || CURRENT_SCHEMA() as STATUS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Sample E-commerce Data\n",
    "\n",
    "We'll generate realistic sample data representing an active e-commerce platform with customers, products, and order transactions over the past 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample e-commerce data for Dynamic Tables demo\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.types import StructType, StructField, IntegerType, StringType, DateType, FloatType\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Generate customers data\n",
    "customers_data = []\n",
    "for i in range(1, 1001):  # 1000 customers\n",
    "    customers_data.append({\n",
    "        'CUSTOMER_ID': i,\n",
    "        'CUSTOMER_NAME': f'Customer_{i}',\n",
    "        'EMAIL': f'customer{i}@email.com',\n",
    "        'SEGMENT': random.choice(['Premium', 'Standard', 'Basic']),\n",
    "        'REGISTRATION_DATE': datetime(2023, 1, 1) + timedelta(days=random.randint(0, 365))\n",
    "    })\n",
    "\n",
    "# Generate products data\n",
    "products_data = []\n",
    "categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports']\n",
    "for i in range(1, 101):  # 100 products\n",
    "    products_data.append({\n",
    "        'PRODUCT_ID': i,\n",
    "        'PRODUCT_NAME': f'Product_{i}',\n",
    "        'CATEGORY': random.choice(categories),\n",
    "        'PRICE': round(random.uniform(10.0, 500.0), 2)\n",
    "    })\n",
    "\n",
    "# Generate orders data (recent 30 days with realistic patterns)\n",
    "orders_data = []\n",
    "order_id = 1\n",
    "base_date = datetime.now() - timedelta(days=30)\n",
    "\n",
    "for day in range(30):\n",
    "    current_date = base_date + timedelta(days=day)\n",
    "    # More orders on weekends, fewer late at night\n",
    "    daily_orders = random.randint(20, 100) if current_date.weekday() < 5 else random.randint(50, 150)\n",
    "    \n",
    "    for _ in range(daily_orders):\n",
    "        order_time = current_date + timedelta(\n",
    "            hours=random.randint(6, 23),\n",
    "            minutes=random.randint(0, 59),\n",
    "            seconds=random.randint(0, 59)\n",
    "        )\n",
    "        \n",
    "        orders_data.append({\n",
    "            'ORDER_ID': order_id,\n",
    "            'CUSTOMER_ID': random.randint(1, 1000),\n",
    "            'PRODUCT_ID': random.randint(1, 100),\n",
    "            'ORDER_DATE': order_time,\n",
    "            'QUANTITY': random.randint(1, 5),\n",
    "            'STATUS': random.choice(['Completed', 'Pending', 'Shipped', 'Cancelled'])\n",
    "        })\n",
    "        order_id += 1\n",
    "\n",
    "# Create and populate base tables\n",
    "customers_df = session.create_dataframe(customers_data)\n",
    "customers_df.write.save_as_table(\"CUSTOMERS\", mode=\"overwrite\")\n",
    "\n",
    "products_df = session.create_dataframe(products_data)\n",
    "products_df.write.save_as_table(\"PRODUCTS\", mode=\"overwrite\")\n",
    "\n",
    "orders_df = session.create_dataframe(orders_data)\n",
    "orders_df.write.save_as_table(\"ORDERS\", mode=\"overwrite\")\n",
    "\n",
    "print(f\"ðŸ“Š Sample data created successfully:\")\n",
    "print(f\"   â€¢ {len(customers_data)} customers\")\n",
    "print(f\"   â€¢ {len(products_data)} products\") \n",
    "print(f\"   â€¢ {len(orders_data)} orders (last 30 days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our sample data is ready for Dynamic Tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "VERIFY_DATA"
   },
   "outputs": [],
   "source": [
    "-- Verify sample data creation\n",
    "SELECT 'CUSTOMERS' as TABLE_NAME, COUNT(*) as ROW_COUNT FROM CUSTOMERS\n",
    "UNION ALL\n",
    "SELECT 'PRODUCTS' as TABLE_NAME, COUNT(*) as ROW_COUNT FROM PRODUCTS  \n",
    "UNION ALL\n",
    "SELECT 'ORDERS' as TABLE_NAME, COUNT(*) as ROW_COUNT FROM ORDERS\n",
    "ORDER BY TABLE_NAME;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Your First Dynamic Table\n",
    "\n",
    "Now let's create our first Dynamic Table that automatically aggregates order data by hour. This table will refresh every hour and maintain real-time hourly analytics without any manual intervention.\n",
    "\n",
    "**Key Features:**\n",
    "- `TARGET_LAG = '1 hour'`: Refreshes automatically within 1 hour of source data changes\n",
    "- Automatic dependency tracking on ORDERS and PRODUCTS tables\n",
    "- Self-maintaining aggregations and calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "CREATE_HOURLY_DYNAMIC_TABLE"
   },
   "outputs": [],
   "source": [
    "-- Create Dynamic Table for hourly order analytics\n",
    "CREATE OR REPLACE DYNAMIC TABLE DT_HOURLY_ORDERS\n",
    "TARGET_LAG = '1 hour'\n",
    "WAREHOUSE = SNOWFLAKE_LEARNING_WH\n",
    "AS\n",
    "SELECT \n",
    "    DATE_TRUNC('HOUR', o.ORDER_DATE) AS ORDER_HOUR,\n",
    "    COUNT(*) AS TOTAL_ORDERS,\n",
    "    SUM(o.QUANTITY * p.PRICE) AS TOTAL_REVENUE,\n",
    "    COUNT(DISTINCT o.CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n",
    "    COUNT(DISTINCT o.PRODUCT_ID) AS UNIQUE_PRODUCTS,\n",
    "    AVG(o.QUANTITY * p.PRICE) AS AVG_ORDER_VALUE\n",
    "FROM ORDERS o\n",
    "JOIN PRODUCTS p ON o.PRODUCT_ID = p.PRODUCT_ID\n",
    "WHERE o.STATUS = 'Completed'\n",
    "GROUP BY DATE_TRUNC('HOUR', o.ORDER_DATE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "VIEW_HOURLY_DATA"
   },
   "outputs": [],
   "source": [
    "-- View recent hourly order data\n",
    "SELECT \n",
    "    ORDER_HOUR,\n",
    "    TOTAL_ORDERS,\n",
    "    ROUND(TOTAL_REVENUE, 2) as TOTAL_REVENUE,\n",
    "    UNIQUE_CUSTOMERS,\n",
    "    ROUND(AVG_ORDER_VALUE, 2) as AVG_ORDER_VALUE\n",
    "FROM DT_HOURLY_ORDERS \n",
    "ORDER BY ORDER_HOUR DESC \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Layered Dynamic Tables\n",
    "\n",
    "One of the most powerful features of Dynamic Tables is automatic dependency management. Let's create a daily summary table that depends on our hourly table. Snowflake will automatically manage the refresh order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "CREATE_DAILY_SUMMARY"
   },
   "outputs": [],
   "source": [
    "-- Create layered Dynamic Table - Daily Summary (depends on hourly data)\n",
    "CREATE OR REPLACE DYNAMIC TABLE DT_DAILY_SUMMARY\n",
    "TARGET_LAG = '4 hours'\n",
    "WAREHOUSE = SNOWFLAKE_LEARNING_WH\n",
    "AS\n",
    "SELECT \n",
    "    DATE(ORDER_HOUR) AS ORDER_DATE,\n",
    "    SUM(TOTAL_ORDERS) AS DAILY_ORDERS,\n",
    "    SUM(TOTAL_REVENUE) AS DAILY_REVENUE,\n",
    "    MAX(UNIQUE_CUSTOMERS) AS PEAK_HOUR_CUSTOMERS,\n",
    "    AVG(AVG_ORDER_VALUE) AS DAILY_AVG_ORDER_VALUE,\n",
    "    COUNT(*) AS ACTIVE_HOURS\n",
    "FROM DT_HOURLY_ORDERS\n",
    "GROUP BY DATE(ORDER_HOUR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "CREATE_CUSTOMER_METRICS"
   },
   "outputs": [],
   "source": [
    "-- Create Advanced Dynamic Table - Customer Lifetime Metrics\n",
    "CREATE OR REPLACE DYNAMIC TABLE DT_CUSTOMER_METRICS\n",
    "TARGET_LAG = '2 hours'\n",
    "WAREHOUSE = SNOWFLAKE_LEARNING_WH\n",
    "AS\n",
    "SELECT \n",
    "    c.CUSTOMER_ID,\n",
    "    c.CUSTOMER_NAME,\n",
    "    c.SEGMENT,\n",
    "    COUNT(DISTINCT o.ORDER_ID) AS TOTAL_ORDERS,\n",
    "    SUM(o.QUANTITY * p.PRICE) AS LIFETIME_VALUE,\n",
    "    AVG(o.QUANTITY * p.PRICE) AS AVG_ORDER_VALUE,\n",
    "    MAX(o.ORDER_DATE) AS LAST_ORDER_DATE,\n",
    "    MIN(o.ORDER_DATE) AS FIRST_ORDER_DATE,\n",
    "    DATEDIFF('day', MIN(o.ORDER_DATE), MAX(o.ORDER_DATE)) AS CUSTOMER_LIFESPAN_DAYS\n",
    "FROM CUSTOMERS c\n",
    "LEFT JOIN ORDERS o ON c.CUSTOMER_ID = o.CUSTOMER_ID AND o.STATUS = 'Completed'\n",
    "LEFT JOIN PRODUCTS p ON o.PRODUCT_ID = p.PRODUCT_ID\n",
    "GROUP BY c.CUSTOMER_ID, c.CUSTOMER_NAME, c.SEGMENT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Dynamic Tables Monitoring & Management\n",
    "\n",
    "Dynamic Tables provide built-in monitoring and management capabilities. Let's explore how to monitor refresh status, suspend/resume operations, and trigger manual refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "MONITOR_DYNAMIC_TABLES"
   },
   "outputs": [],
   "source": [
    "-- Monitor Dynamic Tables status and configuration\n",
    "SELECT \n",
    "    TABLE_NAME,\n",
    "    TARGET_LAG,\n",
    "    IS_SUSPENDED,\n",
    "    REFRESH_MODE,\n",
    "    LAST_SUCCESSFUL_REFRESH,\n",
    "    DATA_TIMESTAMP\n",
    "FROM INFORMATION_SCHEMA.DYNAMIC_TABLES \n",
    "WHERE TABLE_SCHEMA = CURRENT_SCHEMA()\n",
    "ORDER BY TABLE_NAME;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "SUSPEND_RESUME_DEMO"
   },
   "outputs": [],
   "source": [
    "-- Demonstrate suspend and resume operations\n",
    "-- Suspend the hourly orders table\n",
    "ALTER DYNAMIC TABLE DT_HOURLY_ORDERS SUSPEND;\n",
    "\n",
    "-- Check suspension status\n",
    "SELECT TABLE_NAME, IS_SUSPENDED \n",
    "FROM INFORMATION_SCHEMA.DYNAMIC_TABLES \n",
    "WHERE TABLE_NAME = 'DT_HOURLY_ORDERS';\n",
    "\n",
    "-- Resume the table\n",
    "ALTER DYNAMIC TABLE DT_HOURLY_ORDERS RESUME;\n",
    "\n",
    "-- Verify resumption\n",
    "SELECT TABLE_NAME, IS_SUSPENDED \n",
    "FROM INFORMATION_SCHEMA.DYNAMIC_TABLES \n",
    "WHERE TABLE_NAME = 'DT_HOURLY_ORDERS';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "MANUAL_REFRESH"
   },
   "outputs": [],
   "source": [
    "-- Demonstrate manual refresh for immediate updates\n",
    "ALTER DYNAMIC TABLE DT_CUSTOMER_METRICS REFRESH;\n",
    "\n",
    "SELECT 'Manual refresh triggered for DT_CUSTOMER_METRICS' as STATUS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Explore Your Dynamic Tables Data\n",
    "\n",
    "Now let's explore the automatically maintained data in our Dynamic Tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "EXPLORE_DAILY_TRENDS"
   },
   "outputs": [],
   "source": [
    "-- Explore daily business trends\n",
    "SELECT \n",
    "    ORDER_DATE,\n",
    "    DAILY_ORDERS,\n",
    "    ROUND(DAILY_REVENUE, 2) as DAILY_REVENUE,\n",
    "    PEAK_HOUR_CUSTOMERS,\n",
    "    ROUND(DAILY_AVG_ORDER_VALUE, 2) as DAILY_AVG_ORDER_VALUE,\n",
    "    ACTIVE_HOURS\n",
    "FROM DT_DAILY_SUMMARY\n",
    "ORDER BY ORDER_DATE DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "TOP_CUSTOMERS"
   },
   "outputs": [],
   "source": [
    "-- Analyze top customers by lifetime value\n",
    "SELECT \n",
    "    CUSTOMER_NAME,\n",
    "    SEGMENT,\n",
    "    TOTAL_ORDERS,\n",
    "    ROUND(LIFETIME_VALUE, 2) as LIFETIME_VALUE,\n",
    "    ROUND(AVG_ORDER_VALUE, 2) as AVG_ORDER_VALUE,\n",
    "    CUSTOMER_LIFESPAN_DAYS\n",
    "FROM DT_CUSTOMER_METRICS \n",
    "WHERE LIFETIME_VALUE > 0\n",
    "ORDER BY LIFETIME_VALUE DESC \n",
    "LIMIT 15;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Data Sharing with Dynamic Tables\n",
    "\n",
    "Dynamic Tables integrate seamlessly with Snowflake's data sharing capabilities. Let's create a view suitable for sharing that automatically reflects Dynamic Table updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "CREATE_SHARING_VIEW"
   },
   "outputs": [],
   "source": [
    "-- Create a view suitable for data sharing\n",
    "CREATE OR REPLACE VIEW PUBLIC_DAILY_METRICS AS\n",
    "SELECT \n",
    "    ORDER_DATE,\n",
    "    DAILY_ORDERS,\n",
    "    DAILY_REVENUE,\n",
    "    DAILY_AVG_ORDER_VALUE,\n",
    "    CASE \n",
    "        WHEN DAILY_REVENUE > 10000 THEN 'High'\n",
    "        WHEN DAILY_REVENUE > 5000 THEN 'Medium' \n",
    "        ELSE 'Low'\n",
    "    END AS REVENUE_CATEGORY\n",
    "FROM DT_DAILY_SUMMARY\n",
    "WHERE ORDER_DATE >= DATEADD('day', -7, CURRENT_DATE());\n",
    "\n",
    "-- View the shareable data\n",
    "SELECT * FROM PUBLIC_DAILY_METRICS ORDER BY ORDER_DATE DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations! You've successfully implemented and explored Dynamic Tables. Here's what you've learned:\n",
    "\n",
    "### ðŸŽ¯ Core Benefits\n",
    "- **Automatic Data Freshness**: No more complex scheduling or manual refresh processes\n",
    "- **Intelligent Dependency Management**: Snowflake automatically manages refresh order between layered tables\n",
    "- **Built-in Monitoring**: Full visibility into refresh status, performance, and health\n",
    "- **Cost Optimization**: Efficient refresh strategies with TARGET_LAG configuration\n",
    "- **Seamless Integration**: Works perfectly with data sharing, views, and other Snowflake features\n",
    "\n",
    "### ðŸ’¡ Best Practices\n",
    "1. **Set appropriate TARGET_LAG values** based on business requirements (longer = less compute cost)\n",
    "2. **Use layered approaches** for complex transformations to optimize refresh efficiency\n",
    "3. **Monitor refresh patterns** and adjust LAG accordingly for optimal cost/performance balance\n",
    "4. **Leverage automatic dependency management** to avoid redundant refresh operations\n",
    "5. **Suspend tables during maintenance** or low-usage periods to save costs\n",
    "6. **Use manual refresh sparingly** - only when immediate updates are critical\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "- **Production Implementation**: Apply these patterns to your real-world data pipelines\n",
    "- **Advanced Features**: Explore clustering keys, data retention, and security policies\n",
    "- **Cost Monitoring**: Set up alerts and dashboards for refresh cost tracking\n",
    "- **Data Sharing**: Share your Dynamic Tables with external partners for real-time collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Run this final cell to clean up all resources created during this demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "CLEANUP"
   },
   "outputs": [],
   "source": [
    "-- Clean up all created resources\n",
    "DROP VIEW IF EXISTS PUBLIC_DAILY_METRICS;\n",
    "DROP DYNAMIC TABLE IF EXISTS DT_CUSTOMER_METRICS;\n",
    "DROP DYNAMIC TABLE IF EXISTS DT_DAILY_SUMMARY;\n",
    "DROP DYNAMIC TABLE IF EXISTS DT_HOURLY_ORDERS;\n",
    "DROP TABLE IF EXISTS ORDERS;\n",
    "DROP TABLE IF EXISTS PRODUCTS;\n",
    "DROP TABLE IF EXISTS CUSTOMERS;\n",
    "\n",
    "SELECT 'All resources cleaned up successfully!' as CLEANUP_STATUS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "Ready to dive deeper? Explore these resources:\n",
    "\n",
    "- ðŸ“– [Snowflake Dynamic Tables Documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-about)\n",
    "- ðŸŽ¯ [Dynamic Tables Best Practices](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices)  \n",
    "- ðŸ“Š [Monitoring Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-monitoring)\n",
    "- ðŸ”— [Templates Hub](https://app.snowflake.com/templates)\n",
    "\n",
    "**Happy building with Dynamic Tables! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
